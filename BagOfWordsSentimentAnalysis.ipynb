{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgangwar\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading file\n",
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    #remove punctuation\n",
    "    token = [re_punc.sub(' ',w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [w for w in tokens if len(w)>1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'stomach', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'made', 'steve', 'guttenberg', 'star', 'worry', 'make', 'sense', 'see', 'onto', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'say', 'word', 'ably', 'captures', 'dreariness', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'comic', 'oscar', 'winner', 'martin', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'half', 'bad', 'film', 'however', 'good', 'strong', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "#creating tokens on a single file\n",
    "neg = load_doc('txt_sentoken/pos/cv000_29590.txt') \n",
    "token = clean_doc(neg)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_doc_to_vocab(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory,vocab):\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory +'/'+filename\n",
    "        add_doc_to_vocab(path,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36053\n",
      "[('film', 7974), ('one', 4939), ('movie', 4815), ('like', 3193), ('even', 2261), ('good', 2073), ('time', 2039), ('story', 1899), ('would', 1843), ('much', 1823), ('also', 1757), ('get', 1723), ('character', 1699), ('two', 1642), ('characters', 1618), ('first', 1586), ('see', 1553), ('way', 1515), ('well', 1477), ('make', 1418), ('really', 1400), ('little', 1347), ('films', 1338), ('life', 1329), ('plot', 1286), ('people', 1267), ('could', 1248), ('bad', 1246), ('scene', 1240), ('never', 1197), ('best', 1176), ('new', 1139), ('scenes', 1132), ('many', 1129), ('man', 1122), ('know', 1092), ('movies', 1027), ('great', 1011), ('another', 992), ('action', 980), ('love', 975), ('us', 967), ('go', 950), ('director', 947), ('something', 944), ('end', 943), ('still', 935), ('seems', 930), ('back', 921), ('made', 911)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter()\n",
    "process_docs('txt_sentoken/pos',vocab)\n",
    "process_docs('txt_sentoken/neg',vocab)\n",
    "\n",
    "print(len(vocab))\n",
    "# top 50 most common words used in the reviews\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23275\n"
     ]
    }
   ],
   "source": [
    "tokens = [k for k,c in vocab.items() if c>= 2]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving token or vocab in a txt file\n",
    "data = '\\n'.join(tokens)\n",
    "file = open('vocab.txt','w')\n",
    "file.write(data)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23275"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the vocubulary \n",
    "vocab_file = 'vocab.txt'\n",
    "vocab = load_doc(vocab_file)\n",
    "vocab = set(vocab.split())\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load doc, clean and return line of tokens()\n",
    "def doc_to_line(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab,is_train):\n",
    "    neg = process_docs('txt_sentoken/neg',vocab,is_train)\n",
    "    pos = process_docs('txt_sentoken/pos',vocab,is_train)\n",
    "    docs = neg + pos\n",
    "\n",
    "    labels = [0 for _ in range (len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie reviews to bag of words vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 23276) (200, 23276)\n"
     ]
    }
   ],
   "source": [
    "train_docs, ytrain = load_clean_dataset(vocab,True)\n",
    "test_docs, ytest = load_clean_dataset(vocab,False)\n",
    "\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs,mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs,mode='freq')\n",
    "\n",
    "print(Xtrain.shape , Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(n_words):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50,input_shape=(n_words,),activation='relu'))\n",
    "    model.add(Dense(25,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy' ,optimizer='adam',metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 2s - loss: 0.6915 - acc: 0.5394\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6707 - acc: 0.6272\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.5976 - acc: 0.8350\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.4549 - acc: 0.9361\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.2933 - acc: 0.9728\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.1769 - acc: 0.9872\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.1084 - acc: 0.9967\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0690 - acc: 0.9978\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0452 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.0314 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x273c9b50400>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = Xtest.shape[1]\n",
    "model = define_model(n_words)\n",
    "model.fit(Xtrain,ytrain,epochs=10,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.000000\n"
     ]
    }
   ],
   "source": [
    "loss,acc = model.evaluate(Xtest,ytest,verbose=10)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Word Scoring Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts to matrix() function for the Tokenizer in the Keras API provides 4 different\n",
    "methods for scoring words; they are:\n",
    "\n",
    " binary Where words are marked as present (1) or absent (0).\n",
    "\n",
    " count Where the occurrence count for each word is marked as an integer.\n",
    "\n",
    " tfidf Where each word is scored based on their frequency, where words that are common\n",
    "across all documents are penalized.\n",
    "\n",
    " freq Where words are scored based on their frequency of occurrence within the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
    "    scores = list()\n",
    "    n_repeats = 10\n",
    "    n_words = Xtest.shape[1]\n",
    "    for i in range(n_repeats):\n",
    "        # define network\n",
    "        model = define_model(n_words)\n",
    "        model.fit(Xtrain, ytrain, epochs=10, verbose=0)\n",
    "        _, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "        scores.append(acc)\n",
    "        print('%d accuracy: %s' % ((i+1), acc))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary\n",
      "1 accuracy: 0.935\n",
      "2 accuracy: 0.92\n",
      "3 accuracy: 0.935\n",
      "4 accuracy: 0.915\n",
      "5 accuracy: 0.935\n",
      "6 accuracy: 0.925\n",
      "7 accuracy: 0.93\n",
      "8 accuracy: 0.935\n",
      "9 accuracy: 0.915\n",
      "10 accuracy: 0.92\n",
      "count\n",
      "1 accuracy: 0.9\n",
      "2 accuracy: 0.905\n",
      "3 accuracy: 0.89\n",
      "4 accuracy: 0.9\n",
      "5 accuracy: 0.905\n",
      "6 accuracy: 0.89\n",
      "7 accuracy: 0.89\n",
      "8 accuracy: 0.895\n",
      "9 accuracy: 0.905\n",
      "10 accuracy: 0.9\n",
      "tfidf\n",
      "1 accuracy: 0.885\n",
      "2 accuracy: 0.885\n",
      "3 accuracy: 0.885\n",
      "4 accuracy: 0.82\n",
      "5 accuracy: 0.87\n",
      "6 accuracy: 0.865\n",
      "7 accuracy: 0.865\n",
      "8 accuracy: 0.875\n",
      "9 accuracy: 0.865\n",
      "10 accuracy: 0.84\n",
      "freq\n",
      "1 accuracy: 0.925\n",
      "2 accuracy: 0.9\n",
      "3 accuracy: 0.915\n",
      "4 accuracy: 0.91\n",
      "5 accuracy: 0.92\n",
      "6 accuracy: 0.9\n",
      "7 accuracy: 0.92\n",
      "8 accuracy: 0.915\n",
      "9 accuracy: 0.895\n",
      "10 accuracy: 0.915\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "modes = ['binary','count','tfidf','freq']\n",
    "results = DataFrame()\n",
    "for mode in modes:\n",
    "    print(mode)\n",
    "    Xtrain,Xtest = prepare_data(train_docs,test_docs,mode)\n",
    "    results[mode] = evaluate_mode(Xtrain,ytrain,Xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          binary      count      tfidf       freq\n",
      "count  10.000000  10.000000  10.000000  10.000000\n",
      "mean    0.926500   0.898000   0.865500   0.911500\n",
      "std     0.008515   0.006325   0.021009   0.010014\n",
      "min     0.915000   0.890000   0.820000   0.895000\n",
      "25%     0.920000   0.891250   0.865000   0.902500\n",
      "50%     0.927500   0.900000   0.867500   0.915000\n",
      "75%     0.935000   0.903750   0.882500   0.918750\n",
      "max     0.935000   0.905000   0.885000   0.925000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(results.describe())\n",
    "results.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review,vocab,tokenizer,model):\n",
    "    tokens = clean_doc(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    encoded = tokenizer.texts_to_matrix([line],mode='binary')\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 3s - loss: 0.4560 - acc: 0.7878\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0406 - acc: 0.9933\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 5.8979e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 3.6994e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 2.5699e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 1.8647e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 1.4259e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x273c9d4a908>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choosing binary as it performing better than the other two\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
    "# define network\n",
    "n_words = Xtrain.shape[1]\n",
    "model = define_model(n_words)\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [Best movie ever! It was great, I recommend it.]\n",
      "Sentiment: NEGATIVE (52.887%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Best movie ever! It was great, I recommend it.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [This is a bad movie.]\n",
      "Sentiment: NEGATIVE (68.379%)\n"
     ]
    }
   ],
   "source": [
    "text = 'This is a bad movie.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [This movie wasn't worth watching.]\n",
      "Sentiment: NEGATIVE (54.934%)\n"
     ]
    }
   ],
   "source": [
    "text = 'This movie wasn\\'t worth watching.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
