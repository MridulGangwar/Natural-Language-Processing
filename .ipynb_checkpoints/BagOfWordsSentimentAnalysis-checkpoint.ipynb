{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgangwar\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading file\n",
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    #remove punctuation\n",
    "    token = [re_punc.sub(' ',w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [w for w in tokens if len(w)>1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'stomach', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'made', 'steve', 'guttenberg', 'star', 'worry', 'make', 'sense', 'see', 'onto', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'say', 'word', 'ably', 'captures', 'dreariness', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'comic', 'oscar', 'winner', 'martin', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'half', 'bad', 'film', 'however', 'good', 'strong', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "#creating tokens on a single file\n",
    "neg = load_doc('txt_sentoken/pos/cv000_29590.txt') \n",
    "token = clean_doc(neg)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_doc_to_vocab(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory,vocab):\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory +'/'+filename\n",
    "        add_doc_to_vocab(path,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36053\n",
      "[('film', 7974), ('one', 4939), ('movie', 4815), ('like', 3193), ('even', 2261), ('good', 2073), ('time', 2039), ('story', 1899), ('would', 1843), ('much', 1823), ('also', 1757), ('get', 1723), ('character', 1699), ('two', 1642), ('characters', 1618), ('first', 1586), ('see', 1553), ('way', 1515), ('well', 1477), ('make', 1418), ('really', 1400), ('little', 1347), ('films', 1338), ('life', 1329), ('plot', 1286), ('people', 1267), ('could', 1248), ('bad', 1246), ('scene', 1240), ('never', 1197), ('best', 1176), ('new', 1139), ('scenes', 1132), ('many', 1129), ('man', 1122), ('know', 1092), ('movies', 1027), ('great', 1011), ('another', 992), ('action', 980), ('love', 975), ('us', 967), ('go', 950), ('director', 947), ('something', 944), ('end', 943), ('still', 935), ('seems', 930), ('back', 921), ('made', 911)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter()\n",
    "process_docs('txt_sentoken/pos',vocab)\n",
    "process_docs('txt_sentoken/neg',vocab)\n",
    "\n",
    "print(len(vocab))\n",
    "# top 50 most common words used in the reviews\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23275\n"
     ]
    }
   ],
   "source": [
    "tokens = [k for k,c in vocab.items() if c>= 2]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving token or vocab in a txt file\n",
    "data = '\\n'.join(tokens)\n",
    "file = open('vocab.txt','w')\n",
    "file.write(data)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23275"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the vocubulary \n",
    "vocab_file = 'vocab.txt'\n",
    "vocab = load_doc(vocab_file)\n",
    "vocab = set(vocab.split())\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load doc, clean and return line of tokens()\n",
    "def doc_to_line(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab,is_train):\n",
    "    neg = process_docs('txt_sentoken/neg',vocab,is_train)\n",
    "    pos = process_docs('txt_sentoken/pos',vocab,is_train)\n",
    "    docs = neg + pos\n",
    "\n",
    "    labels = [0 for _ in range (len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie reviews to bag of words vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 23276) (200, 23276)\n"
     ]
    }
   ],
   "source": [
    "train_docs, ytrain = load_clean_dataset(vocab,True)\n",
    "test_docs, ytest = load_clean_dataset(vocab,False)\n",
    "\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs,mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs,mode='freq')\n",
    "\n",
    "print(Xtrain.shape , Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(n_words):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50,input_shape=(n_words,),activation='relu'))\n",
    "    model.add(Dense(25,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy' ,optimizer='adam',metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                1163850   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 1,165,151\n",
      "Trainable params: 1,165,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.6920 - acc: 0.5761\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6756 - acc: 0.8100\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6198 - acc: 0.9189\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.5046 - acc: 0.9400\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.3603 - acc: 0.9600\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.2408 - acc: 0.9739\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.1570 - acc: 0.9883\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.1050 - acc: 0.9961\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0717 - acc: 0.9972\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.0503 - acc: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x238f71d9e10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = Xtest.shape[1]\n",
    "model = define_model(n_words)\n",
    "model.fit(Xtrain,ytrain,epochs=10,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.000000\n"
     ]
    }
   ],
   "source": [
    "loss,acc = model.evaluate(Xtest,ytest,verbose=10)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Word Scoring Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts to matrix() function for the Tokenizer in the Keras API provides 4 different\n",
    "methods for scoring words; they are:\n",
    "\n",
    " binary Where words are marked as present (1) or absent (0).\n",
    "\n",
    " count Where the occurrence count for each word is marked as an integer.\n",
    "\n",
    " tfidf Where each word is scored based on their frequency, where words that are common\n",
    "across all documents are penalized.\n",
    "\n",
    " freq Where words are scored based on their frequency of occurrence within the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
    "    scores = list()\n",
    "    n_repeats = 10\n",
    "    n_words = Xtest.shape[1]\n",
    "    for i in range(n_repeats):\n",
    "        # define network\n",
    "        model = define_model(n_words)\n",
    "        model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "        _, acc = model.evaluate(Xtest, ytest, verbose=2)\n",
    "        scores.append(acc)\n",
    "        print('%d accuracy: %s' % ((i+1), acc))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary\n",
      "1 accuracy: 0.93\n",
      "2 accuracy: 0.925\n",
      "count\n",
      "1 accuracy: 0.905\n",
      "2 accuracy: 0.89\n",
      "tfidf\n",
      "1 accuracy: 0.87\n",
      "2 accuracy: 0.865\n",
      "freq\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "modes = ['binary','count','tfidf','freq']\n",
    "results = DataFrame()\n",
    "for mode in modes:\n",
    "    print(mode)\n",
    "    Xtrain,Xtest = prepare_data(train_docs,test_docs,mode)\n",
    "    results[mode] = evaluate_model(Xtrain,ytrain,Xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.describe())\n",
    "results.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
