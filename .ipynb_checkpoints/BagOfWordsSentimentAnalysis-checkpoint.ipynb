{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading file\n",
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will prepare the data using the following method:\n",
    "\n",
    " Split tokens on white space.\n",
    "\n",
    " Remove all punctuation from words.\n",
    "\n",
    " Remove all words that are not purely comprised of alphabetical characters.\n",
    "\n",
    " Remove all words that are known stop words.\n",
    "\n",
    " Remove all words that have a length <= 1 character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    #remove punctuation\n",
    "    token = [re_punc.sub(' ',w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [w for w in tokens if len(w)>1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'stomach', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'made', 'steve', 'guttenberg', 'star', 'worry', 'make', 'sense', 'see', 'onto', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'say', 'word', 'ably', 'captures', 'dreariness', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'comic', 'oscar', 'winner', 'martin', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'half', 'bad', 'film', 'however', 'good', 'strong', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "#creating tokens on a single file\n",
    "neg = load_doc('txt_sentoken/pos/cv000_29590.txt') \n",
    "token = clean_doc(neg)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining a Vocabulary\n",
    "\n",
    "It is important to define a vocabulary of known words when using a bag-of-words model. The\n",
    "more words, the larger the representation of documents, therefore it is important to constrain\n",
    "the words to only those believed to be predictive. This is di\u000ecult to know beforehand and often\n",
    "it is important to test different hypotheses about how to construct a useful vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_doc_to_vocab(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory,vocab):\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory +'/'+filename\n",
    "        add_doc_to_vocab(path,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36053\n",
      "[('film', 7974), ('one', 4939), ('movie', 4815), ('like', 3193), ('even', 2261), ('good', 2073), ('time', 2039), ('story', 1899), ('would', 1843), ('much', 1823), ('also', 1757), ('get', 1723), ('character', 1699), ('two', 1642), ('characters', 1618), ('first', 1586), ('see', 1553), ('way', 1515), ('well', 1477), ('make', 1418), ('really', 1400), ('little', 1347), ('films', 1338), ('life', 1329), ('plot', 1286), ('people', 1267), ('could', 1248), ('bad', 1246), ('scene', 1240), ('never', 1197), ('best', 1176), ('new', 1139), ('scenes', 1132), ('many', 1129), ('man', 1122), ('know', 1092), ('movies', 1027), ('great', 1011), ('another', 992), ('action', 980), ('love', 975), ('us', 967), ('go', 950), ('director', 947), ('something', 944), ('end', 943), ('still', 935), ('seems', 930), ('back', 921), ('made', 911)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter()\n",
    "process_docs('txt_sentoken/pos',vocab)\n",
    "process_docs('txt_sentoken/neg',vocab)\n",
    "\n",
    "print(len(vocab))\n",
    "# top 50 most common words used in the reviews\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23275\n"
     ]
    }
   ],
   "source": [
    "tokens = [k for k,c in vocab.items() if c>= 2]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving token or vocab in a txt file\n",
    "data = '\\n'.join(tokens)\n",
    "file = open('vocab.txt','w')\n",
    "file.write(data)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23275"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the vocubulary \n",
    "vocab_file = 'vocab.txt'\n",
    "vocab = load_doc(vocab_file)\n",
    "vocab = set(vocab.split())\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag of Words\n",
    "\n",
    "any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document. The intuition is that documents are similar if they have similar content. As the vocabulary size increases, so does the vector representation of documents.\n",
    "\n",
    "A more sophisticated approach is to create a vocabulary of grouped words. This both changes the scope of the vocabulary and allows the bag-of-words to capture a little bit more meaning from the document. In this approach, each word or token is called a gram. Creating a vocabulary of two-word pairs is, in turn, called a bigram model. Again, only the bigrams that appear in the corpus are modeled, not all possible bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load doc, clean and return line of tokens()\n",
    "def doc_to_line(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab,is_train):\n",
    "    neg = process_docs('txt_sentoken/neg',vocab,is_train)\n",
    "    pos = process_docs('txt_sentoken/pos',vocab,is_train)\n",
    "    docs = neg + pos\n",
    "\n",
    "    labels = [0 for _ in range (len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie reviews to bag of words vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "200\n",
      "(1800, 23276) (200, 23276)\n"
     ]
    }
   ],
   "source": [
    "train_docs, ytrain = load_clean_dataset(vocab,True)\n",
    "test_docs, ytest = load_clean_dataset(vocab,False)\n",
    "\n",
    "print(len(train_docs))\n",
    "print(len(test_docs))\n",
    "\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs,mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs,mode='freq')\n",
    "\n",
    "print(Xtrain.shape , Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(n_words):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50,input_shape=(n_words,),activation='relu'))\n",
    "    model.add(Dense(25,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy' ,optimizer='adam',metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 2s - loss: 0.6927 - acc: 0.5878\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6881 - acc: 0.5250\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6679 - acc: 0.7272\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6068 - acc: 0.8383\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.4873 - acc: 0.9356\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.3451 - acc: 0.9622\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.2284 - acc: 0.9828\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.1529 - acc: 0.9872\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.1028 - acc: 0.9950\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.0714 - acc: 0.9972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1936eb4c940>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = Xtest.shape[1]\n",
    "model = define_model(n_words)\n",
    "model.fit(Xtrain,ytrain,epochs=10,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.000000\n"
     ]
    }
   ],
   "source": [
    "loss,acc = model.evaluate(Xtest,ytest,verbose=10)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Word Scoring Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts to matrix() function for the Tokenizer in the Keras API provides 4 different\n",
    "methods for scoring words; they are:\n",
    "\n",
    " binary Where words are marked as present (1) or absent (0).\n",
    "\n",
    " count Where the occurrence count for each word is marked as an integer.\n",
    "\n",
    " tfidf Where each word is scored based on their frequency, where words that are common\n",
    "across all documents are penalized.\n",
    "\n",
    " freq Where words are scored based on their frequency of occurrence within the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
    "    scores = list()\n",
    "    n_repeats = 10\n",
    "    n_words = Xtest.shape[1]\n",
    "    for i in range(n_repeats):\n",
    "        # define network\n",
    "        model = define_model(n_words)\n",
    "        model.fit(Xtrain, ytrain, epochs=10, verbose=0)\n",
    "        _, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "        scores.append(acc)\n",
    "        print('%d accuracy: %s' % ((i+1), acc))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary\n",
      "1 accuracy: 0.915\n",
      "2 accuracy: 0.915\n",
      "3 accuracy: 0.92\n",
      "4 accuracy: 0.92\n",
      "5 accuracy: 0.92\n",
      "6 accuracy: 0.93\n",
      "7 accuracy: 0.915\n",
      "8 accuracy: 0.935\n",
      "9 accuracy: 0.925\n",
      "10 accuracy: 0.915\n",
      "count\n",
      "1 accuracy: 0.915\n",
      "2 accuracy: 0.885\n",
      "3 accuracy: 0.88\n",
      "4 accuracy: 0.89\n",
      "5 accuracy: 0.875\n",
      "6 accuracy: 0.905\n",
      "7 accuracy: 0.905\n",
      "8 accuracy: 0.88\n",
      "9 accuracy: 0.91\n",
      "10 accuracy: 0.885\n",
      "tfidf\n",
      "1 accuracy: 0.875\n",
      "2 accuracy: 0.87\n",
      "3 accuracy: 0.875\n",
      "4 accuracy: 0.875\n",
      "5 accuracy: 0.87\n",
      "6 accuracy: 0.855\n",
      "7 accuracy: 0.89\n",
      "8 accuracy: 0.865\n",
      "9 accuracy: 0.89\n",
      "10 accuracy: 0.88\n",
      "freq\n",
      "1 accuracy: 0.91\n",
      "2 accuracy: 0.91\n",
      "3 accuracy: 0.92\n",
      "4 accuracy: 0.915\n",
      "5 accuracy: 0.915\n",
      "6 accuracy: 0.915\n",
      "7 accuracy: 0.92\n",
      "8 accuracy: 0.915\n",
      "9 accuracy: 0.9\n",
      "10 accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "modes = ['binary','count','tfidf','freq']\n",
    "results = DataFrame()\n",
    "for mode in modes:\n",
    "    print(mode)\n",
    "    Xtrain,Xtest = prepare_data(train_docs,test_docs,mode)\n",
    "    results[mode] = evaluate_mode(Xtrain,ytrain,Xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          binary      count      tfidf       freq\n",
      "count  10.000000  10.000000  10.000000  10.000000\n",
      "mean    0.921000   0.893000   0.874500   0.913000\n",
      "std     0.006992   0.014376   0.010659   0.005869\n",
      "min     0.915000   0.875000   0.855000   0.900000\n",
      "25%     0.915000   0.881250   0.870000   0.910000\n",
      "50%     0.920000   0.887500   0.875000   0.915000\n",
      "75%     0.923750   0.905000   0.878750   0.915000\n",
      "max     0.935000   0.915000   0.890000   0.920000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGV9JREFUeJzt3X9wXeWd3/H3Z2U7gGMcgrPqgo3tdkgrrXHYoppQnKy03rAm2cQNbBKr2QS26nq3XZwpE7YxIwaIOxqcDXS2XbxpndiBQCqGeLqpi70Y1r2XhIQQmwUbbMXEdZ1YONOQpPEiYBbL+faPexRfXwvryDry1b3P5zWj0fnxnEfPeXTvR0fPvfc5igjMzCwNv1LvBpiZ2dnj0DczS4hD38wsIQ59M7OEOPTNzBLi0DczS4hD38wsIQ59M7OEOPTNzBIyrd4NqDVnzpxYsGBBvZsxpldffZWZM2fWuxlNw/1ZLPdncRqlL5955pmfRMQ7xio35UJ/wYIF7Nq1q97NGFO5XKazs7PezWga7s9iuT+L0yh9KekHecp5eMfMLCEOfTOzhDj0zcwS4tA3M0uIQ9/MLCEOfTOzhDj0zcwS4tA3M0vIlPtw1lQgqdD6fB9iM5sqfKU/iogY82v+Zx7JVc6Bb2ZTiUPfzCwhDn0zs4Q49M3MEuLQNzNLiEPfzCwhDn0zs4Q49M3MEpIr9CUtl7Rf0gFJa0bZP1/SDkl7JJUlza3a/oyk5yTtlfTHRZ+AmZnlN2boS2oB1gPXAu1At6T2mmJ3A1+JiMXAWuCubPuPgH8eEZcDVwJrJF1UVOPNzGx88lzpLwEORMTBiHgDeAhYUVOmHdiRLZdG9kfEGxHx99n2t+T8eWZmNknyhPDFwOGq9cFsW7XdwPXZ8oeBWZIuBJA0T9KerI7PRcSRiTXZzMzOVJ4J10abfax2QplbgHsl3Qh8A3gJGAaIiMPA4mxY5+uSNkfE/z3pB0irgFUAra2tlMvl8ZxD3TRKOxvB0NCQ+7NA7s/iNFtf5gn9QWBe1fpc4KSr9ezq/ToASW8Fro+Io7VlJO0F3gNsrtm3AdgA0NHREZ2dneM7i3p4dCsN0c4GUS6X3Z8Fcn8Wp9n6Ms/wzk7gUkkLJc0AVgJbqgtImiNppK5bgU3Z9rmSzs2WLwCuBvYX1XgzMxufMUM/IoaBm4DtwADwcETslbRW0oeyYp3AfkkvAq1AX7a9DXha0m7gCeDuiHi+4HMwM7Occt1EJSK2Adtqtt1etbyZmiGbbPvjwOIJttHMzArit1CamSXEoW9mlhCHvplZQhz6ZmYJceibmSXEoW9mlhCHvplZQhz6ZmYJceibmSXEoW9mlpBc0zA0i3d99jGOvn6ssPoWrNlaSD2zz53O7juuKaQuM7PTSSr0j75+jEPrPlBIXUVOt1rUHw8zs7F4eMfMLCEOfTOzhDj0zcwS4tA3M0tIrtCXtFzSfkkHJK0ZZf98STsk7ZFUljQ32365pKck7c32fazoEzAzs/zGDH1JLcB64FqgHeiW1F5T7G7gKxGxGFgL3JVtfw34ZET8OrAc+HNJbyuq8WZmNj55rvSXAAci4mBEvAE8BKyoKdMO7MiWSyP7I+LFiPh+tnwE+DHwjiIabmZm45fnffoXA4er1geBK2vK7AauB/4T8GFglqQLI+KnIwUkLQFmAP+79gdIWgWsAmhtbaVcLo/jFPKb1baGy+4/ZXTqzN1fTDWz2qBcnllMZQ1qaGho0n7vKXJ/5tPV1VVofaVSqdD6JkVEnPYL+Ajwpar1TwB/UVPmIuC/A89SCf5BYHbV/l8D9gPvHuvnXXHFFTFZ5n/mkcLqKpVKhdVVZLsaVZH9ae7PIjXK8xPYFWPka0TkutIfBOZVrc8FjtT84TgCXAcg6a3A9RFxNFs/H9gK3BYR3xn/nyUzMytKnjH9ncClkhZKmgGsBLZUF5A0R9JIXbcCm7LtM4C/ovIi79eKa7aZmZ2JMUM/IoaBm4DtwADwcETslbRW0oeyYp3AfkkvAq1AX7b9o8B7gRslPZd9XV70SZiZWT65JlyLiG3Atpptt1ctbwY2j3Lcg8CDE2yjmZkVxJ/INTNLiEPfzCwhDn0zs4Q49M3MEuLQNzNLiEPfzCwhDn0zs4QkdWN0yHcT8h987ncL/ZnzP/PIaffPPnd6oT9vqpFUWF2VKUbMxnbZ/ZcVUs+sNgqdqPH5G54vrK4zkVToH1r3gXwF140dLOVymc7Ozok1KBF5gnrBmq35fz9mObwysK6Qx1SRz/U8F52TzcM7ZmYJceibmSXEoW9mlhCHvplZQhz6ZmYJceibmSUkV+hLWi5pv6QDkk55w6qk+ZJ2SNojqSxpbtW+RyX9XNLp36xuZmaTbszQl9QCrAeuBdqBbkntNcXupnJLxMXAWuCuqn2fp3IzdTMzq7M8V/pLgAMRcTAi3gAeAlbUlGkHdmTLper9EbEDeKWAtpqZ2QTlCf2LgcNV64PZtmq7geuz5Q8DsyRdOPHmmZlZkfJMwzDaxCm1n6u/BbhX0o3AN4CXgOG8jZC0ClgF0NraSrlcznto3QwNDTVEOxuJ+7M4fnxWjDXtwdmeZ2vm9Po/zvOE/iAwr2p9LnCkukBEHAGuA5D0VuD6iDiatxERsQHYANDR0RGNMKeN594p2KNb3Z8F8uMTDnXmKJTgPFt5hnd2ApdKWihpBrAS2FJdQNIcSSN13QpsKraZZmZWhDFDPyKGgZuA7cAA8HBE7JW0VtKHsmKdwH5JLwKtQN/I8ZK+CXwNWCZpUNLvFHwOZmaWU66plSNiG7CtZtvtVcubgc1vcux7JtJAMzMrjj+Ra2aWEIe+mVlCHPpmZglx6JuZJcShb2aWEIe+mVlCHPpmZgnJ9T59s9G867OPcfT1Y4XVN9Y8KXnNPnc6u++4ppC6zJqNQ9/O2NHXj3Fo3QcKqavI+U2K+uNh1ow8vGNmlhCHvplZQhz6ZmYJceibmSXEoW9mlhCHvplZQhz6ZmYJyRX6kpZL2i/pgKQ1o+yfL2mHpD2SypLmVu27QdL3s68bimy8mZmNz5ihL6kFWA9cC7QD3ZLaa4rdDXwlIhYDa4G7smPfDtwBXAksAe6QdEFxzTczmxz9/f0sWrSIZcuWsWjRIvr7++vdpELk+UTuEuBARBwEkPQQsALYV1WmHbg5Wy4BX8+Wfwd4PCJ+lh37OLAcaI7eM7Om1N/fT29vLxs3buT48eO0tLTQ09MDQHd3d51bNzF5hncuBg5XrQ9m26rtBq7Plj8MzJJ0Yc5jzcymlL6+PjZu3EhXVxfTpk2jq6uLjRs30tfXV++mTVieK32Nsi1q1m8B7pV0I/AN4CVgOOexSFoFrAJobW2lXC7naFZ9DQ0NNUQ7J1tRfVB0f6b+u/Hjc2IGBgY4fvw45XL5l315/PhxBgYGGr5f84T+IDCvan0ucKS6QEQcAa4DkPRW4PqIOCppEOisObZc+wMiYgOwAaCjoyOKmnhrMhU5QVjDenRrYX1QaH8W2K5G5cfnxLS1tdHS0kJnZ+cv+7JUKtHW1tbw/ZpneGcncKmkhZJmACuBLdUFJM2RNFLXrcCmbHk7cI2kC7IXcK/JtpmZTVm9vb309PRQKpUYHh6mVCrR09NDb29vvZs2YWNe6UfEsKSbqIR1C7ApIvZKWgvsiogtVK7m75IUVIZ3/iQ79meS/gOVPxwAa0de1DUzm6pGXqxdvXo1AwMDtLW10dfX1/Av4kLO+fQjYhuwrWbb7VXLm4HNb3LsJk5c+ZuZNYTu7m66u7ubbqjMn8g1M0uIQ9/MLCEOfTOzhDj0zcwS4tA3M0uIQ9/MLCEOfTOzhDj0zcwS4tA3M0uIQ9/MLCEOfTOzhDj0zcwS4tA3M0uIQ9/MLCEOfTOzhDj0zcwSkiv0JS2XtF/SAUlrRtl/iaSSpGcl7ZH0/mz7DElflvS8pN2SOgtuv5mZjcOYoS+pBVgPXAu0A92S2muK3QY8HBG/QeUeun+Zbf9DgIi4DHgfcE/VvXTNzOwsyxPAS4ADEXEwIt4AHgJW1JQJ4PxseTZwJFtuB3YARMSPgZ8DHRNttJmZnZk898i9GDhctT4IXFlT5k7gMUmrgZnAb2fbdwMrJD0EzAOuyL5/t/pgSauAVQCtra2Uy+VxnUQ9DA0NNUQ7J1tRfVB0fzbr76arq6vQ+kqlUqH1NaNme67nCX2Nsi1q1ruB+yLiHklXAQ9IWkTlhuhtwC7gB8C3geFTKovYAGwA6OjoiEa4CXGz3Sz5jDy6tbA+KLQ/C2zXVBNR+9Qb3YI1Wzm07gOT3Jo0NNtzPU/oD1K5Oh8xlxPDNyN6gOUAEfGUpHOAOdmQzs0jhSR9G/j+hFpsZmZnLM+Y/k7gUkkLJc2g8kLtlpoyPwSWAUhqA84BXpZ0nqSZ2fb3AcMRsa+w1puZ2biMeaUfEcOSbgK2Ay3ApojYK2ktsCsitgCfBr4o6WYqQz83RkRI+lVgu6RfAC8Bn5i0MzEzszHlGd4hIrYB22q23V61vA+4epTjDgH/eGJNNDOzovg982ZmCcl1pW82mllta7js/lM+oH3m7i+mmlltAH7nitloHPp2xl4ZWFfY2wKLfFvcgjVbC6nHrBl5eMfMLCEOfTOzhDj0zcwS4tA3M0uIQ9/MLCEOfTOzhDj0zcwS4tA3M0uIQ9/MLCEOfTOzhDj0zcwS4tA3M0tIrtCXtFzSfkkHJJ0yraKkSySVJD0raY+k92fbp0u6X9LzkgYk3Vr0CZiZWX5jzrIpqQVYD7yPyv1yd0raUnPbw9uAhyPiC5LaqdxwZQHwEeAtEXGZpPOAfZL6s5urWBModEbLR4upa/a50wupx6wZ5ZlaeQlwICIOAkh6CFgBVId+AOdny7M5ceP0AGZKmgacC7wB/F0B7bYpoKhplaHyx6PI+sxsdHmGdy4GDletD2bbqt0J/L6kQSpX+auz7ZuBV4EfUbl5+t0R8bOJNNjMzM5cnit9jbItata7gfsi4h5JVwEPSFpE5b+E48BFwAXANyX9zch/Db/8AdIqYBVAa2sr5XJ5fGdRB0NDQw3Rzkbi/iyW+7MYzfZczxP6g8C8qvW5nBi+GdEDLAeIiKcknQPMAf4l8GhEHAN+LOlbQAdwUuhHxAZgA0BHR0cUdQelyVTknZ4MeHSr+7NI7s/CNNtzPc/wzk7gUkkLJc0AVgJbasr8EFgGIKkNOAd4Odv+W6qYCbwb+F5RjTczs/EZ80o/IoYl3QRsB1qATRGxV9JaYFdEbAE+DXxR0s1Uhn5ujIiQtB74MvAClWGiL0fEnsk6GbNG9q7PPsbR148VVl9R76yafe50dt9xTSF1Wf3lujF6RGyj8gJt9bbbq5b3AVePctwQlbdtmtkYjr5+zDeat0nnT+SamSXEoW9mlhCHvplZQhz6ZmYJceibmSXEoW9mlhCHvplZQnK9T99sIqTRpm8apdznxi4TUTvtU/OY1baGy+4/5XYVZ+7+YqqZ1QbgGVCbhUPfJl2eoG62+U3OxCsD6/zhLJt0Ht4xM0uIQ9/MLCEOfTOzhDj0zcwS4tA3M0uIQ9/MLCEOfTOzhOQKfUnLJe2XdEDSKZ8ekXSJpJKkZyXtkfT+bPvHJT1X9fULSZcXfRJmZpbPmKEvqQVYD1wLtAPdktprit0GPBwRv0HlHrp/CRARX42IyyPicuATwKGIeK7IEzAzs/zyXOkvAQ5ExMGIeAN4CFhRUyaA87Pl2cCRUerpBvrPtKFmZjZxeaZhuBg4XLU+CFxZU+ZO4DFJq4GZwG+PUs/HOPWPhZmZnUV5Qn+02bJqJ1PpBu6LiHskXQU8IGlRRPwCQNKVwGsR8cKoP0BaBawCaG1tpVwu521/3QwNDTVEOxuF+7Oi0HluHi2mrpnTSfp302yPzTyhPwjMq1qfy6nDNz3AcoCIeErSOcAc4MfZ/pWcZmgnIjYAGwA6OjqiESbe8gRhxXJ/wqHO4upasGZrYZO3pa7ZHpt5xvR3ApdKWihpBpUA31JT5ofAMgBJbcA5wMvZ+q8AH6HyWoCZmdXRmKEfEcPATcB2YIDKu3T2Slor6UNZsU8DfyhpN5Ur+hvjxHy67wUGI+Jg8c03M7PxyDWffkRsA7bVbLu9ankfcPWbHFsG3n3mTTQzs6L4E7lmZglx6JuZJcShb2aWEIe+mVlCHPpmZglx6JuZJcShb2aWkFzv0zezqUEabSqsNyn7ubHLnPgMpaXCV/pmDSQicn2VSqVc5Sw9Dn0zs4Q49M3MEuLQNzNLiEPfzCwhDn0zs4Q49M3MEuLQNzNLSK7Ql7Rc0n5JByStGWX/JZJKkp6VtEfS+6v2LZb0lKS9kp7P7p9rZmZ1MOYnciW1AOuB91G5SfpOSVuyu2WNuI3KbRS/IKmdyl22FkiaBjwIfCIidku6EDhW+FmYmVkuea70lwAHIuJgRLxB5QbnK2rKBHB+tjwbOJItXwPsiYjdABHx04g4PvFmm5nZmcgT+hcDh6vWB7Nt1e4Efl/SIJWr/NXZ9ncCIWm7pL+V9O8n2F4zM5uAPBOujTbDU+2kHd3AfRFxj6SrgAckLcrqXwr8M+A1YIekZyJix0k/QFoFrAJobW2lXC6P7yzqYGhoqCHa2Sjcn8Vyfxan2foyT+gPAvOq1udyYvhmRA+wHCAinsperJ2THftERPwEQNI24J8CJ4V+RGwANgB0dHREZ2fnuE/kbCuXyzRCOxuF+7NY7s/iNFtf5hne2QlcKmmhpBnASmBLTZkfAssAJLUB5wAvA9uBxZLOy17U/U1gH2ZmVhdjXulHxLCkm6gEeAuwKSL2SloL7IqILcCngS9KupnK0M+NUZm39f9J+o9U/nAEsC0itk7WyZiZ2enluolKRGyj8gJt9bbbq5b3AVe/ybEPUnnbppmZ1Zk/kWtmlhCHvplZQhz6ZmYJceibmSXEoW/WRPr7+1m0aBHLli1j0aJF9Pf317tJNsXkeveOmU19/f399Pb2snHjRo4fP05LSws9PT0AdHd317l1NlX4St+sSfT19bFx40a6urqYNm0aXV1dbNy4kb6+vno3zaYQh75ZkxgYGGDp0qUnbVu6dCkDAwN1apFNRQ59sybR1tbGk08+edK2J598kra2tjq1yKYih75Zk+jt7aWnp4dSqcTw8DClUomenh56e3vr3TSbQvxCrlmTGHmxdvXq1QwMDNDW1kZfX59fxLWTOPTNmkh3dzfd3d1NNx2wFcfDO2ZmCXHom5klxKFvZpYQh76ZWUIc+mZmCVHlroZTh6SXgR/Uux05zAF+Uu9GNBH3Z7Hcn8VplL6cHxHvGKvQlAv9RiFpV0R01LsdzcL9WSz3Z3GarS89vGNmlhCHvplZQhz6Z25DvRvQZNyfxXJ/Fqep+tJj+mZmCfGVvplZQpIOfUkLJL0wyvYvSWqvR5vs9CT9O0nn1bsd9SLpbZL+bdX65yXtzb7/saRPjnLMSY9zSf2S9ki6+Wy1eyqT9ClJA5K+Wu+2nA1JD+9IWgA8EhGLJqn+aRExPBl1p0rSIaAjIhrhfdOFq33MSvo74B0R8fd5jpH0D4CnI2L+5Le2MUj6HnBtRPyfqm1N+9xN+ko/M03S/dmVz2ZJ50kqS+oAkDQkqU/SbknfkdSabf+gpKclPSvpb6q23ylpg6THgK9I+qaky0d+mKRvSVpclzM9SyR9MuvP3ZIekDRf0o5s2w5Jl2Tl7pP0e1XHDWXfO7PfwWZJ35P0VVV8CrgIKEkq1efs6m4d8I8kPSfpcWAm8LSkj2WPvVsAJF2R9f9TwJ9UHf8Y8KvZ8e85+82fWiT9F+AfAlskHa157rZk/0HtzB67f5QdI0n3StonaaukbdWP4ykvIpL9AhYAAVydrW8CbgHKVK4myfZ/MFv+M+C2bPkCTvyn9K+Be7LlO4FngHOz9RuAP8+W3wnsqvd5T3Kf/jqwH5iTrb8d+J/ADdn6vwK+ni3fB/xe1bFD2fdO4Cgwl8qFyVPA0mzfoZG6U/zKHrMv1PZZtnwncEu2vAf4zWz58yPH1B7vrxOPqVGeu6uqnu9vAXYBC4HrgMeBFioXIT+vfhxP9S9f6cPhiPhWtvwgsLRm/xvAI9nyM1SeNFAJpO2Sngf+lErYjdgSEa9ny18DflfSdCqBd1+hrZ96fgvYHNnwS0T8DLgK+G/Z/gc4tY9H892IGIyIXwDPcaLfbQySZgNvi4gnsk0P1LM9Dab6uXsN8ElJzwFPAxcClwLvBfoj4nhEHAH+V32aemYc+pUr+dOtH4vsTz1wnBN3G/sL4N6IuAz4I+CcqmNe/WVlEa9RuSpYAXyUE+HXrMSpfVhrZP8w2WNQkoAZVWWqx6ir+93Glud3YKN7tWpZwOqIuDz7WhgRj2X7GrZ/HfpwiaSrsuVu4Mmcx80GXsqWbxij7JeA/wzszK58m9kO4KOSLgSQ9Hbg28DKbP/HOdHHh4ArsuUVwPQc9b8CzCqqsQ1ozPOPiJ8DRyWN/Ef18UlvVXPaDvyb7L90JL1T0kzgG8DKbMz/14CuejZyvHz1BAPADZL+K/B94AvAB3McdyfwNUkvAd+hMtY3qoh4JnuXxZcn3typLSL2SuoDnpB0HHgW+BSwSdKfAi8Df5AV/yLwPyR9l8ofi1dHq7PGBuCvJf0oIhrqyVaEiPhp9maAF4C/Pk3RP6DS569RCS8bvy9RGVb82+w/0ZeBfwH8FZVhzOeBF4En3qyCqSjpt2yeLZIuovLi8D/JxqjNrElIuo/KW2I317steXh4Z5JlH5Z5Guh14JtZvflK38wsIb7SNzNLiEPfzCwhDn0zs4Q49M3MEuLQNzNLiEPfzCwh/x9rQSqHFvYR8QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(results.describe())\n",
    "results.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review,vocab,tokenizer,model):\n",
    "    tokens = clean_doc(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    encoded = tokenizer.texts_to_matrix([line],mode='binary')\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 3s - loss: 0.4920 - acc: 0.7672\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0499 - acc: 0.9928\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.2350e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 4.8513e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 3.4009e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 2.5307e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 1.9315e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1937318e160>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choosing binary as it performing better than the other two\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
    "# define network\n",
    "n_words = Xtrain.shape[1]\n",
    "model = define_model(n_words)\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [Best movie ever! It was great, I recommend it.]\n",
      "Sentiment: POSITIVE (50.421%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Best movie ever! It was great, I recommend it.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [This is a bad movie.]\n",
      "Sentiment: NEGATIVE (67.394%)\n"
     ]
    }
   ],
   "source": [
    "text = 'This is a bad movie.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [This movie wasn't worth watching.]\n",
      "Sentiment: NEGATIVE (54.018%)\n"
     ]
    }
   ],
   "source": [
    "text = 'This movie wasn\\'t worth watching.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
