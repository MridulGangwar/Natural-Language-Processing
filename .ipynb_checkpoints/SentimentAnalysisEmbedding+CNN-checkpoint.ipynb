{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab,is_train):\n",
    "    neg = process_docs('txt_sentoken/neg',vocab,is_train)\n",
    "    pos = process_docs('txt_sentoken/pos',vocab,is_train)\n",
    "    docs = neg + pos\n",
    "\n",
    "    labels = [0 for _ in range (len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training CNN with Embedding layer\n",
    "\n",
    "Convolutional neural networks (CNN) are effective at document classification,\n",
    "namely because they are able to pick out salient features (e.g. tokens or sequences of tokens) in\n",
    "a way that is invariant to their position within the input sequences.\n",
    "\n",
    "Networks with convolutional and pooling layers are useful for classification tasks in\n",
    "which we expect to find strong local clues regarding class membership, but these\n",
    "clues can appear in different places in the input. [...] We would like to learn that\n",
    "certain sequences of words are good indicators of the topic, and do not necessarily\n",
    "care where they appear in the document. Convolutional and pooling layers allow\n",
    "the model to learn to find such local indicators, regardless of their position.\n",
    "\n",
    "A word embedding is a way of representing text where each word in\n",
    "the vocabulary is represented by a real valued vector in a high-dimensional space. The vectors\n",
    "are learned in such a way that words that have similar meanings will have similar representation\n",
    "in the vector space (close in the vector space). This is a more expressive representation for text\n",
    "than more classical methods like bag-of-words, where relationships between words or tokens are\n",
    "ignored, or forced in bigram and trigram approaches.\n",
    "\n",
    "The Keras Embedding\n",
    "layer requires integer inputs where each integer maps to a single token that has a specific\n",
    "real-valued vector representation within the embedding. These vectors are random at the\n",
    "beginning of training, but during training become meaningful to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size,max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,100,input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23275"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab,False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(train_docs))\n",
    "print(len(test_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 23276\n"
     ]
    }
   ],
   "source": [
    "#The Embedding layer requires the specification of the vocabulary\n",
    "#size, the size of the real-valued vector space, and the maximum length of input documents. The\n",
    "#vocabulary size is the total number of words in our vocabulary, plus one for unknown words.\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 1277\n"
     ]
    }
   ],
   "source": [
    "#We also need to ensure that all documents have the same length. This is a\n",
    "#requirement of Keras for eficient computation. We could truncate reviews to the smallest size\n",
    "#or zero-pad (pad with the value 0) reviews to the maximum length, or some hybrid. In this case,\n",
    "#we will pad all reviews to the length of the longest review in the training dataset. \n",
    "\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1277, 100)         2327600   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1270, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 635, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20320)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                203210    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,556,453\n",
      "Trainable params: 2,556,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " - 11s - loss: 0.6926 - acc: 0.5261\n",
      "Epoch 2/10\n",
      " - 11s - loss: 0.4897 - acc: 0.7950\n",
      "Epoch 3/10\n",
      " - 11s - loss: 0.0778 - acc: 0.9761\n",
      "Epoch 4/10\n",
      " - 10s - loss: 0.0078 - acc: 0.9994\n",
      "Epoch 5/10\n",
      " - 11s - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 6/10\n",
      " - 11s - loss: 0.0020 - acc: 0.9994\n",
      "Epoch 7/10\n",
      " - 11s - loss: 0.0015 - acc: 0.9994\n",
      "Epoch 8/10\n",
      " - 11s - loss: 0.0012 - acc: 0.9994\n",
      "Epoch 9/10\n",
      " - 11s - loss: 9.5622e-04 - acc: 0.9994\n",
      "Epoch 10/10\n",
      " - 11s - loss: 7.4264e-04 - acc: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29763bcc748>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "model = define_model(vocab_size,max_length)\n",
    "model.fit(Xtrain,ytrain,epochs=10,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.000000\n",
      "Test Accuracy: 87.000000\n"
     ]
    }
   ],
   "source": [
    "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
    "\n",
    "_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    "# evaluate model on test dataset\n",
    "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "    line = clean_doc(review, vocab)\n",
    "    # encode and pad review\n",
    "    padded = encode_docs(tokenizer, max_length, [line])\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
      "Sentiment: POSITIVE (63.093%)\n",
      "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
      "Sentiment: POSITIVE (59.113%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
